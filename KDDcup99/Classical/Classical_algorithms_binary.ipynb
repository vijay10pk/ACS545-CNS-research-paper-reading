{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6344efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, classification_report,auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eda772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('../dataset/traindata.csv', index_col = 0)\n",
    "testdata = pd.read_csv('../dataset/traindata.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ba9c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['normal.', 'buffer_overflow.', 'loadmodule.', 'perl.', 'neptune.',\n",
       "       'smurf.', 'guess_passwd.', 'pod.', 'teardrop.', 'portsweep.',\n",
       "       'ipsweep.', 'land.', 'ftp_write.', 'back.', 'imap.', 'satan.',\n",
       "       'phf.', 'nmap.', 'multihop.', 'warezmaster.', 'warezclient.',\n",
       "       'spy.', 'rootkit.'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39023abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['target'].replace(['normal.','buffer_overflow.', 'loadmodule.', 'perl.', 'neptune.',\n",
    "       'smurf.', 'guess_passwd.', 'pod.', 'teardrop.', 'portsweep.',\n",
    "       'ipsweep.', 'land.', 'ftp_write.', 'back.', 'imap.', 'satan.',\n",
    "       'phf.', 'nmap.', 'multihop.', 'warezmaster.', 'warezclient.',\n",
    "       'spy.', 'rootkit.'], [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da0a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['target'].replace(['normal.','buffer_overflow.', 'loadmodule.', 'perl.', 'neptune.',\n",
    "       'smurf.', 'guess_passwd.', 'pod.', 'teardrop.', 'portsweep.',\n",
    "       'ipsweep.', 'land.', 'ftp_write.', 'back.', 'imap.', 'satan.',\n",
    "       'phf.', 'nmap.', 'multihop.', 'warezmaster.', 'warezclient.',\n",
    "       'spy.', 'rootkit.'], [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27507ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263bb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['protocol_type', 'service', 'flag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train = traindata.select_dtypes(exclude=[np.number])\n",
    "print(train.columns)\n",
    "for feature in train.columns:\n",
    "    traindata[feature] = LabelEncoder().fit_transform(traindata[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "989ce670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['protocol_type', 'service', 'flag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "test = testdata.select_dtypes(exclude=[np.number])\n",
    "print(test.columns)\n",
    "for feature in test.columns:\n",
    "    testdata[feature] = LabelEncoder().fit_transform(testdata[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e715d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD = traindata.groupby('target', group_keys=False).apply(lambda x: x.sample(frac=0.05))\n",
    "testD = testdata.groupby('target', group_keys=False).apply(lambda x: x.sample(frac=0.07))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5192e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainD.iloc[:,0:41]\n",
    "Y = trainD.iloc[:,41]\n",
    "C = testD.iloc[:,41]\n",
    "T = testD.iloc[:,0:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c712803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a679c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a60d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.array(trainX)\n",
    "trainlabel = np.array(Y)\n",
    "\n",
    "testdata = np.array(testT)\n",
    "testlabel = np.array(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "433be158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(traindata, trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2dde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cd81d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('res')\n",
    "os.chmod('res', 0o777)  # gives read/write/execute permission to all users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ffeb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('res/predictedLR.txt', predicted, fmt='%01d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "864f2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968\n",
      "0.989\n",
      "Accuracy\n",
      "0.9851652641624014\n",
      "precision\n",
      "0.9922351836469356\n",
      "recall\n",
      "0.9892697681117673\n",
      "f-score\n",
      "0.9907502569373073\n",
      "fpr\n",
      "0.989\n",
      "tpr\n",
      "0.968\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "\n",
    "cm = confusion_matrix(expected, predicted)\n",
    "# print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(expected, predicted))\n",
    "print(\"precision\")\n",
    "print(precision_score(expected, predicted ,average='binary'))\n",
    "print(\"recall\")\n",
    "print(recall_score(expected, predicted ,average='binary'))\n",
    "print(\"f-score\")\n",
    "print(f1_score(expected, predicted , average='binary'))\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c603e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n"
     ]
    }
   ],
   "source": [
    "# fit a Naive Bayes model to the data\n",
    "model = GaussianNB()\n",
    "model.fit(traindata, trainlabel)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa8fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('res/predictedNB.txt', predicted, fmt='%01d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b70f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971\n",
      "0.985\n",
      "Accuracy\n",
      "0.9822735027905497\n",
      "precision\n",
      "0.9927786043473528\n",
      "recall\n",
      "0.9850928993230592\n",
      "f-score\n",
      "0.9889208191002911\n",
      "fpr\n",
      "0.985\n",
      "tpr\n",
      "0.971\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "\n",
    "cm = confusion_matrix(expected, predicted)\n",
    "# print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(expected, predicted))\n",
    "print(\"precision\")\n",
    "print(precision_score(expected, predicted ,average='binary'))\n",
    "print(\"recall\")\n",
    "print(recall_score(expected, predicted ,average='binary'))\n",
    "print(\"f-score\")\n",
    "print(f1_score(expected, predicted , average='binary'))\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73691aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praveenvenkatachalam/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(traindata, trainlabel)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1da8c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('res/predictedKNN.txt', predicted, fmt='%01d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d54177b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995\n",
      "0.999\n",
      "Accuracy\n",
      "0.998091437494578\n",
      "precision\n",
      "0.998847677349658\n",
      "recall\n",
      "0.9987757453550339\n",
      "f-score\n",
      "0.998811710057254\n",
      "fpr\n",
      "0.999\n",
      "tpr\n",
      "0.995\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "\n",
    "cm = confusion_matrix(expected, predicted)\n",
    "# print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(expected, predicted))\n",
    "print(\"precision\")\n",
    "print(precision_score(expected, predicted ,average='binary'))\n",
    "print(\"recall\")\n",
    "print(recall_score(expected, predicted ,average='binary'))\n",
    "print(\"f-score\")\n",
    "print(f1_score(expected, predicted , average='binary'))\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70c84fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(traindata, trainlabel)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedDT.txt', predicted, fmt='%01d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3baca219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997\n",
      "0.999\n",
      "Accuracy\n",
      "0.9987276249963853\n",
      "precision\n",
      "0.9992797464707577\n",
      "recall\n",
      "0.9991358202506121\n",
      "f-score\n",
      "0.9992077781778897\n",
      "fpr\n",
      "0.999\n",
      "tpr\n",
      "0.997\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "\n",
    "cm = confusion_matrix(expected, predicted)\n",
    "# print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(expected, predicted))\n",
    "print(\"precision\")\n",
    "print(precision_score(expected, predicted ,average='binary'))\n",
    "print(\"recall\")\n",
    "print(recall_score(expected, predicted ,average='binary'))\n",
    "print(\"f-score\")\n",
    "print(f1_score(expected, predicted , average='binary'))\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e7ac713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "model.fit(traindata, trainlabel)\n",
    "\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedABoost.txt', predicted, fmt='%01d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04ac90a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997\n",
      "0.999\n",
      "Accuracy\n",
      "0.9986119545415112\n",
      "precision\n",
      "0.9992436793200317\n",
      "recall\n",
      "0.9990277977819386\n",
      "f-score\n",
      "0.9991357268896972\n",
      "fpr\n",
      "0.999\n",
      "tpr\n",
      "0.997\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "\n",
    "cm = confusion_matrix(expected, predicted)\n",
    "# print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(expected, predicted))\n",
    "print(\"precision\")\n",
    "print(precision_score(expected, predicted ,average='binary'))\n",
    "print(\"recall\")\n",
    "print(recall_score(expected, predicted ,average='binary'))\n",
    "print(\"f-score\")\n",
    "print(f1_score(expected, predicted , average='binary'))\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b7a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model = model.fit(traindata, trainlabel)\n",
    "\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedRF.txt', predicted, fmt='%01d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d90f52e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.999\n",
      "Accuracy\n",
      "0.9993348948844741\n",
      "precision\n",
      "0.9997838694571521\n",
      "recall\n",
      "0.9993878726775169\n",
      "f-score\n",
      "0.9995858318477302\n",
      "fpr\n",
      "0.999\n",
      "tpr\n",
      "0.999\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "\n",
    "cm = confusion_matrix(expected, predicted)\n",
    "# print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(expected, predicted))\n",
    "print(\"precision\")\n",
    "print(precision_score(expected, predicted ,average='binary'))\n",
    "print(\"recall\")\n",
    "print(recall_score(expected, predicted ,average='binary'))\n",
    "print(\"f-score\")\n",
    "print(f1_score(expected, predicted , average='binary'))\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa328a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "X_train = traindata\n",
    "y_train = trainlabel\n",
    "X_test = testdata\n",
    "y_test = testlabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4b8bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praveenvenkatachalam/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/praveenvenkatachalam/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/praveenvenkatachalam/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/praveenvenkatachalam/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/praveenvenkatachalam/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.978 (+/-0.003) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.402 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.974 (+/-0.007) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.978 (+/-0.003) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.974 (+/-0.007) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.974 (+/-0.007) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.975 (+/-0.007) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.974 (+/-0.007) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.974 (+/-0.007) for {'C': 1, 'kernel': 'linear'}\n",
      "0.976 (+/-0.008) for {'C': 10, 'kernel': 'linear'}\n",
      "0.976 (+/-0.008) for {'C': 100, 'kernel': 'linear'}\n",
      "0.978 (+/-0.007) for {'C': 1000, 'kernel': 'linear'}\n",
      "----------------------------------------------\n",
      "cross-validation accuracy of train data set\n",
      "[0.97791913 0.40154245 0.97434309 0.97794489 0.97445675 0.97434309\n",
      " 0.97459319 0.97445675 0.97434287 0.97580172 0.97644561 0.97838379]\n",
      "----------------------------------------------\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "confusion matrix\n",
      "[[ 6668   141]\n",
      " [  260 27512]]\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      6809\n",
      "           1       0.99      0.99      0.99     27772\n",
      "\n",
      "    accuracy                           0.99     34581\n",
      "   macro avg       0.98      0.98      0.98     34581\n",
      "weighted avg       0.99      0.99      0.99     34581\n",
      "\n",
      "\n",
      "***************************************************************************\n",
      "for now\n",
      "accuracy score\n",
      "0.9884040368988751\n",
      "precision\n",
      "0.9949010957219832\n",
      "recall\n",
      "0.9906380527149647\n",
      "F-score\n",
      "0.9927649977447002\n",
      "best parameters\n",
      "{'C': 1000, 'kernel': 'linear'}\n",
      "***************************************************************************\n",
      "==============================================\n",
      "[[ 6668   141]\n",
      " [  260 27512]]\n",
      "0.979\n",
      "0.991\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.943 (+/-0.007) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.989 (+/-0.003) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.943 (+/-0.007) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.989 (+/-0.001) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.989 (+/-0.003) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.990 (+/-0.002) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.989 (+/-0.001) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.989 (+/-0.002) for {'C': 1, 'kernel': 'linear'}\n",
      "0.987 (+/-0.005) for {'C': 10, 'kernel': 'linear'}\n",
      "0.985 (+/-0.005) for {'C': 100, 'kernel': 'linear'}\n",
      "0.986 (+/-0.003) for {'C': 1000, 'kernel': 'linear'}\n",
      "----------------------------------------------\n",
      "cross-validation accuracy of train data set\n",
      "[0.94283198 0.5        0.98920104 0.94293486 0.98865926 0.98920104\n",
      " 0.9897398  0.98865926 0.98948384 0.98664386 0.98491604 0.9863724 ]\n",
      "----------------------------------------------\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "confusion matrix\n",
      "[[ 6742    67]\n",
      " [  349 27423]]\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      6809\n",
      "           1       1.00      0.99      0.99     27772\n",
      "\n",
      "    accuracy                           0.99     34581\n",
      "   macro avg       0.97      0.99      0.98     34581\n",
      "weighted avg       0.99      0.99      0.99     34581\n",
      "\n",
      "\n",
      "***************************************************************************\n",
      "for now\n",
      "accuracy score\n",
      "0.9879702726930973\n",
      "precision\n",
      "0.9975627500909422\n",
      "recall\n",
      "0.987433386144318\n",
      "F-score\n",
      "0.9924722232275344\n",
      "best parameters\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "***************************************************************************\n",
      "==============================================\n",
      "[[ 6742    67]\n",
      " [  349 27423]]\n",
      "0.990\n",
      "0.987\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"cross-validation accuracy of train data set\")\n",
    "    print(means)\n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    #print(\"accuracy score\")\n",
    "    #print(accuracy_score(y_true, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"Classification report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    print(\"***************************************************************************\")\n",
    "    print(\"for now\")\n",
    "    print(\"accuracy score\")\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(\"precision\")\n",
    "    print(precision_score(y_true, y_pred , average=\"binary\"))\n",
    "    print(\"recall\")\n",
    "    print(recall_score(y_true, y_pred , average=\"binary\"))\n",
    "    print(\"F-score\")\n",
    "    print(f1_score(y_true, y_pred , average=\"binary\"))\n",
    "    print(\"best parameters\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"***************************************************************************\")\n",
    "    predicted = y_pred\n",
    "    expected = y_true\n",
    "    cm = metrics.confusion_matrix(expected, predicted)\n",
    "    print(\"==============================================\")\n",
    "    print(cm)\n",
    "    tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "    fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "    print(\"%.3f\" %tpr)\n",
    "    print(\"%.3f\" %fpr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae0e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
